{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "```\n",
    "2DIP_exercise/\n",
    "│-- data/             # Contains images & videos\n",
    "│   │-- input/        # 1 image and 1 video for each phase respectively\n",
    "│   │-- output/       # All output images/videos must be stored here\n",
    "│-- notebooks/        # Jupyter Notebooks for each phase\n",
    "│   │-- part1.ipynb   # Image processing & feature extraction\n",
    "│   │-- part2.ipynb   # Optical flow, object detection and tracking \n",
    "│-- README.md         # Project instructions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "inputs = os.path.join(base_path, 'data','input')\n",
    "outputs = os.path.join(base_path, 'data','output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Code for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path):\n",
    "    # Re-open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB for matplotlib\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "def display_video(video_path):\n",
    "    \n",
    "    frames = get_frames(video_path)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    im = ax.imshow(np.zeros_like(frames[0]))\n",
    "    ax.axis('off')\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_array(frame)\n",
    "        return [im]\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=frames, interval=50, blit=True, repeat=False)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Analyze movement patterns in a video sequence. **(6)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compute dense optical flow for each frame in a video of a moving crowd. **(2)**\n",
    "\n",
    "b) Visualize the movement patterns in 2 different ways. **(2+2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def optical_flow(video_path, output_path1, output_path2):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Get frame dimensions and fps\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define output video writers\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_hsv = cv2.VideoWriter(output_path1, fourcc, fps, (width, height))\n",
    "    out_draw = cv2.VideoWriter(output_path2, fourcc, fps, (width, height))\n",
    "\n",
    "    ret, frame1 = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading first frame\")\n",
    "        return\n",
    "    prev_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Prepare HSV image for visualization\n",
    "    hsv = np.zeros_like(frame1)\n",
    "    hsv[...,1] = 255  # saturation set to max\n",
    "\n",
    "    while True:\n",
    "        ret, frame2 = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate dense optical flow using Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, \n",
    "                                            None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Compute magnitude and angle of flow\n",
    "        mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "\n",
    "        # Set hue according to flow direction\n",
    "        hsv[...,0] = ang * 180 / np.pi / 2\n",
    "        # Set value according to flow magnitude (normalize to 0-255)\n",
    "        hsv[...,2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # Convert HSV to BGR for visualization\n",
    "        flow_bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # Draw flow vectors on the original frame for output2\n",
    "        step = 16\n",
    "        vis = frame2.copy()\n",
    "        for y in range(0, height, step):\n",
    "            for x in range(0, width, step):\n",
    "                fx, fy = flow[y, x]\n",
    "                cv2.arrowedLine(vis, (x, y), (int(x+fx), int(y+fy)), (0, 255, 0), 1, tipLength=0.3)\n",
    "\n",
    "        # Write frames to output videos\n",
    "        out_hsv.write(flow_bgr)\n",
    "        out_draw.write(vis)\n",
    "\n",
    "        prev_gray = gray\n",
    "\n",
    "    cap.release()\n",
    "    out_hsv.release()\n",
    "    out_draw.release()\n",
    "    print(\"Optical flow processing completed and videos saved.\")\n",
    "\n",
    "# Example usage:\n",
    "# optical_flow('input_video.mp4', 'output_flow_hsv.mp4', 'output_flow_arrows.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optical flow processing completed and videos saved.\n"
     ]
    }
   ],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "output_path1 = os.path.join(outputs, 'optical_flow_1.mp4')  # Output visualization video path\n",
    "output_path2 = os.path.join(outputs, 'optical_flow_2.mp4')  # Output visualization video path\n",
    "\n",
    "optical_flow(video_path, output_path1, output_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path1)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path2)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Identify and track a moving object in a video sequence. **(9)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Detect an object using template matching. The output would be the first frame where it appears, with a bounding box around the detected object. **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def locate_object(video_path, template_path, output_path):\n",
    "    # Read the template image and convert to grayscale\n",
    "    template = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)\n",
    "    w, h = template.shape[::-1]\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return None\n",
    "\n",
    "    last_frame = None\n",
    "    last_top_left = None\n",
    "    last_bottom_right = None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Perform template matching\n",
    "        res = cv2.matchTemplate(gray_frame, template, cv2.TM_CCOEFF_NORMED)\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "        # Threshold for detection - adjust as needed\n",
    "        threshold = 0.7\n",
    "        if max_val >= threshold:\n",
    "            last_top_left = max_loc\n",
    "            last_bottom_right = (max_loc[0] + w, max_loc[1] + h)\n",
    "            last_frame = frame.copy()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if last_frame is not None and last_top_left and last_bottom_right:\n",
    "        # Draw rectangle around detected template\n",
    "        cv2.rectangle(last_frame, last_top_left, last_bottom_right, (0, 255, 0), 2)\n",
    "        # Save the result image\n",
    "        cv2.imwrite(output_path, last_frame)\n",
    "        return last_frame\n",
    "    else:\n",
    "        print(\"Template not found in any frame.\")\n",
    "        return None\n",
    "\n",
    "# Usage example:\n",
    "# image = locate_object(video_path, template_path, output_path)\n",
    "# display_images(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'detected_object.jpg')  # Output video path\n",
    "\n",
    "image = locate_object(video_path, template_path, output_path)\n",
    "display_images(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement a Kalman filter to predict the object's position in subsequent frames. **(5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def track(video_path, template_path, output_path):\n",
    "    # Load template\n",
    "    template = cv2.imread(template_path, cv2.IMREAD_COLOR)\n",
    "    if template is None:\n",
    "        raise ValueError(\"Template image not found.\")\n",
    "    template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "    w, h = template_gray.shape[::-1]\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file.\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Kalman filter setup\n",
    "    kf = cv2.KalmanFilter(4, 2)\n",
    "    kf.transitionMatrix = np.array([[1, 0, 1, 0],\n",
    "                                    [0, 1, 0, 1],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, 0, 1]], np.float32)\n",
    "    kf.measurementMatrix = np.array([[1, 0, 0, 0],\n",
    "                                     [0, 1, 0, 0]], np.float32)\n",
    "    kf.processNoiseCov = np.eye(4, dtype=np.float32) * 1e-1\n",
    "    kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * 1e-1\n",
    "\n",
    "    detected = False\n",
    "    threshold = 0.6\n",
    "    redetect_threshold = 0.6\n",
    "    base_search_margin = 150\n",
    "    max_search_margin = 400  # maximum margin for adaptive search window\n",
    "    missed_count = 0\n",
    "    max_missed = 30\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Adaptive search margin during tracking (increases with missed count)\n",
    "        if missed_count > 5:\n",
    "            search_margin = min(base_search_margin + 20 * (missed_count - 5), max_search_margin)\n",
    "        else:\n",
    "            search_margin = base_search_margin\n",
    "\n",
    "        # If lost tracking or missed too many frames, do multi-scale re-detection\n",
    "        if not detected or missed_count >= max_missed:\n",
    "            scales = [1.0, 0.9, 1.1, 0.8, 1.2]  # scales to try for multi-scale template matching\n",
    "            best_val = -1\n",
    "            best_loc = None\n",
    "            best_scale = 1.0\n",
    "            best_w, best_h = w, h\n",
    "\n",
    "            for scale in scales:\n",
    "                scaled_template = cv2.resize(template_gray, (int(w * scale), int(h * scale)))\n",
    "                if scaled_template.shape[0] > frame_gray.shape[0] or scaled_template.shape[1] > frame_gray.shape[1]:\n",
    "                    continue\n",
    "\n",
    "                result = cv2.matchTemplate(frame_gray, scaled_template, cv2.TM_CCOEFF_NORMED)\n",
    "                min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "                if max_val > best_val:\n",
    "                    best_val = max_val\n",
    "                    best_loc = max_loc\n",
    "                    best_scale = scale\n",
    "                    best_w, best_h = scaled_template.shape[::-1]\n",
    "\n",
    "            print(f\"Redetection max_val: {best_val:.3f}\")\n",
    "\n",
    "            if best_val >= redetect_threshold:\n",
    "                x, y = best_loc\n",
    "                # Initialize Kalman state with center of detected bounding box\n",
    "                kf.statePre = np.array([[np.float32(x + best_w / 2)],\n",
    "                                        [np.float32(y + best_h / 2)],\n",
    "                                        [0],\n",
    "                                        [0]], dtype=np.float32)\n",
    "                kf.statePost = kf.statePre.copy()\n",
    "                detected = True\n",
    "                missed_count = 0\n",
    "                # Update template to detected scale and region\n",
    "                template_gray = frame_gray[y:y+best_h, x:x+best_w].copy()\n",
    "                w, h = best_w, best_h\n",
    "                cv2.rectangle(frame, (x, y), (x + best_w, y + best_h), (255, 0, 0), 2)\n",
    "                print(\"Redetection at:\", (x, y), \"Scale:\", best_scale)\n",
    "                out.write(frame)\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Redetection failed.\")\n",
    "                out.write(frame)\n",
    "                continue\n",
    "\n",
    "        # Prediction step\n",
    "        prediction = kf.predict()\n",
    "        pred_x, pred_y = int(prediction[0, 0]), int(prediction[1, 0])\n",
    "        pred_x = np.clip(pred_x, 0, width - w)\n",
    "        pred_y = np.clip(pred_y, 0, height - h)\n",
    "\n",
    "        search_top = max(0, pred_y - search_margin)\n",
    "        search_bottom = min(height, pred_y + search_margin + h)\n",
    "        search_left = max(0, pred_x - search_margin)\n",
    "        search_right = min(width, pred_x + search_margin + w)\n",
    "        search_roi = frame_gray[search_top:search_bottom, search_left:search_right]\n",
    "\n",
    "        measurement = None\n",
    "        if search_roi.shape[0] >= h and search_roi.shape[1] >= w:\n",
    "            result = cv2.matchTemplate(search_roi, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "            print(f\"Tracking max_val: {max_val:.3f}\")\n",
    "\n",
    "            if max_val >= threshold:\n",
    "                meas_x = search_left + max_loc[0] + w / 2\n",
    "                meas_y = search_top + max_loc[1] + h / 2\n",
    "                measurement = np.array([[np.float32(meas_x)], [np.float32(meas_y)]])\n",
    "                matched_region = frame_gray[search_top + max_loc[1]:search_top + max_loc[1] + h,\n",
    "                                            search_left + max_loc[0]:search_left + max_loc[0] + w]\n",
    "                # Update template only if confidence is very high to avoid drift\n",
    "                if matched_region.shape == template_gray.shape and max_val >= 0.90:\n",
    "                    template_gray = matched_region.copy()\n",
    "\n",
    "        if measurement is not None:\n",
    "            kf.correct(measurement)\n",
    "            missed_count = 0\n",
    "            tracked_x = int(measurement[0, 0] - w / 2)\n",
    "            tracked_y = int(measurement[1, 0] - h / 2)\n",
    "            print(f\"Predicted: ({pred_x}, {pred_y}), Measured: ({tracked_x}, {tracked_y})\")\n",
    "        else:\n",
    "            missed_count += 1\n",
    "            tracked_x = pred_x\n",
    "            tracked_y = pred_y\n",
    "            print(f\"Predicted: ({pred_x}, {pred_y}), Measured: None\")\n",
    "\n",
    "        if missed_count >= max_missed:\n",
    "            print(\"Tracking lost. Reinitializing detection...\")\n",
    "            detected = False\n",
    "            continue\n",
    "\n",
    "        cv2.rectangle(frame, (tracked_x, tracked_y), (tracked_x + w, tracked_y + h), (0, 255, 0), 2)\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redetection max_val: 0.797\n",
      "Redetection at: (1318, 0) Scale: 1.0\n",
      "Tracking max_val: 0.978\n",
      "Predicted: (1465, 112), Measured: (1315, 6)\n",
      "Tracking max_val: 0.985\n",
      "Predicted: (1463, 115), Measured: (1313, 10)\n",
      "Tracking max_val: 0.985\n",
      "Predicted: (1460, 122), Measured: (1310, 15)\n",
      "Tracking max_val: 0.989\n",
      "Predicted: (1455, 130), Measured: (1307, 20)\n",
      "Tracking max_val: 0.986\n",
      "Predicted: (1451, 136), Measured: (1304, 26)\n",
      "Tracking max_val: 0.980\n",
      "Predicted: (1448, 143), Measured: (1300, 32)\n",
      "Tracking max_val: 0.989\n",
      "Predicted: (1443, 149), Measured: (1297, 38)\n",
      "Tracking max_val: 0.975\n",
      "Predicted: (1440, 155), Measured: (1294, 45)\n",
      "Tracking max_val: 0.973\n",
      "Predicted: (1437, 163), Measured: (1291, 52)\n",
      "Tracking max_val: 0.967\n",
      "Predicted: (1434, 170), Measured: (1288, 60)\n",
      "Tracking max_val: 0.981\n",
      "Predicted: (1431, 179), Measured: (1285, 66)\n",
      "Tracking max_val: 0.893\n",
      "Predicted: (1428, 185), Measured: (1281, 72)\n",
      "Tracking max_val: 0.951\n",
      "Predicted: (1424, 190), Measured: (1278, 80)\n",
      "Tracking max_val: 0.941\n",
      "Predicted: (1421, 198), Measured: (1274, 85)\n",
      "Tracking max_val: 0.981\n",
      "Predicted: (1417, 203), Measured: (1271, 91)\n",
      "Tracking max_val: 0.985\n",
      "Predicted: (1414, 209), Measured: (1267, 98)\n",
      "Tracking max_val: 0.984\n",
      "Predicted: (1410, 216), Measured: (1263, 104)\n",
      "Tracking max_val: 0.983\n",
      "Predicted: (1406, 222), Measured: (1259, 112)\n",
      "Tracking max_val: 0.984\n",
      "Predicted: (1402, 230), Measured: (1255, 119)\n",
      "Tracking max_val: 0.979\n",
      "Predicted: (1398, 238), Measured: (1251, 127)\n",
      "Tracking max_val: 0.984\n",
      "Predicted: (1394, 246), Measured: (1247, 135)\n",
      "Tracking max_val: 0.977\n",
      "Predicted: (1390, 254), Measured: (1243, 143)\n",
      "Tracking max_val: 0.971\n",
      "Predicted: (1385, 262), Measured: (1239, 150)\n",
      "Tracking max_val: 0.987\n",
      "Predicted: (1381, 269), Measured: (1234, 158)\n",
      "Tracking max_val: 0.980\n",
      "Predicted: (1376, 277), Measured: (1230, 166)\n",
      "Tracking max_val: 0.977\n",
      "Predicted: (1372, 285), Measured: (1225, 174)\n",
      "Tracking max_val: 0.987\n",
      "Predicted: (1367, 293), Measured: (1220, 182)\n",
      "Tracking max_val: 0.982\n",
      "Predicted: (1362, 301), Measured: (1215, 190)\n",
      "Tracking max_val: 0.977\n",
      "Predicted: (1357, 309), Measured: (1210, 199)\n",
      "Tracking max_val: 0.975\n",
      "Predicted: (1352, 319), Measured: (1205, 208)\n",
      "Tracking max_val: 0.981\n",
      "Predicted: (1347, 328), Measured: (1199, 217)\n",
      "Tracking max_val: 0.975\n",
      "Predicted: (1340, 337), Measured: (1193, 226)\n",
      "Tracking max_val: 0.976\n",
      "Predicted: (1334, 346), Measured: (1187, 235)\n",
      "Tracking max_val: 0.980\n",
      "Predicted: (1328, 355), Measured: (1181, 244)\n",
      "Tracking max_val: 0.984\n",
      "Predicted: (1322, 365), Measured: (1175, 253)\n",
      "Tracking max_val: 0.978\n",
      "Predicted: (1316, 374), Measured: (1169, 262)\n",
      "Tracking max_val: 0.983\n",
      "Predicted: (1309, 383), Measured: (1162, 271)\n",
      "Tracking max_val: 0.974\n",
      "Predicted: (1302, 392), Measured: (1155, 281)\n",
      "Tracking max_val: 0.985\n",
      "Predicted: (1295, 402), Measured: (1148, 290)\n",
      "Tracking max_val: 0.982\n",
      "Predicted: (1288, 411), Measured: (1141, 299)\n",
      "Tracking max_val: 0.986\n",
      "Predicted: (1281, 420), Measured: (1134, 308)\n",
      "Tracking max_val: 0.971\n",
      "Predicted: (1274, 429), Measured: (1127, 318)\n",
      "Tracking max_val: 0.978\n",
      "Predicted: (1266, 439), Measured: (1119, 328)\n",
      "Tracking max_val: 0.980\n",
      "Predicted: (1258, 449), Measured: (1111, 338)\n",
      "Tracking max_val: 0.978\n",
      "Predicted: (1250, 459), Measured: (1103, 348)\n",
      "Tracking max_val: 0.960\n",
      "Predicted: (1242, 469), Measured: (1095, 361)\n",
      "Tracking max_val: 0.985\n",
      "Predicted: (1234, 483), Measured: (1086, 373)\n",
      "Tracking max_val: 0.970\n",
      "Predicted: (1224, 496), Measured: (1077, 384)\n",
      "Tracking max_val: 0.977\n",
      "Predicted: (1215, 507), Measured: (1068, 397)\n",
      "Tracking max_val: 0.979\n",
      "Predicted: (1206, 520), Measured: (1059, 409)\n",
      "Tracking max_val: 0.970\n",
      "Predicted: (1197, 533), Measured: (1050, 421)\n",
      "Tracking max_val: 0.978\n",
      "Predicted: (1188, 545), Measured: (1040, 434)\n",
      "Tracking max_val: 0.974\n",
      "Predicted: (1177, 558), Measured: (1030, 446)\n",
      "Tracking max_val: 0.973\n",
      "Predicted: (1167, 570), Measured: (1020, 459)\n",
      "Tracking max_val: 0.972\n",
      "Predicted: (1157, 583), Measured: (1009, 472)\n",
      "Tracking max_val: 0.970\n",
      "Predicted: (1145, 596), Measured: (998, 485)\n",
      "Tracking max_val: 0.979\n",
      "Predicted: (1134, 609), Measured: (987, 499)\n",
      "Tracking max_val: 0.966\n",
      "Predicted: (1123, 624), Measured: (975, 513)\n",
      "Tracking max_val: 0.965\n",
      "Predicted: (1110, 638), Measured: (963, 527)\n",
      "Tracking max_val: 0.971\n",
      "Predicted: (1098, 652), Measured: (951, 542)\n",
      "Tracking max_val: 0.966\n",
      "Predicted: (1086, 668), Measured: (939, 557)\n",
      "Tracking max_val: 0.965\n",
      "Predicted: (1074, 683), Measured: (927, 572)\n",
      "Tracking max_val: 0.961\n",
      "Predicted: (1062, 698), Measured: (914, 588)\n",
      "Tracking max_val: 0.956\n",
      "Predicted: (1048, 715), Measured: (901, 603)\n",
      "Tracking max_val: 0.965\n",
      "Predicted: (1035, 730), Measured: (887, 619)\n",
      "Tracking max_val: 0.956\n",
      "Predicted: (1020, 746), Measured: (873, 635)\n",
      "Tracking max_val: 0.961\n",
      "Predicted: (1006, 762), Measured: (859, 652)\n",
      "Tracking max_val: 0.947\n",
      "Predicted: (992, 780), Measured: (844, 671)\n",
      "Tracking max_val: 0.961\n",
      "Predicted: (976, 800), Measured: (829, 689)\n",
      "Tracking max_val: 0.962\n",
      "Predicted: (961, 818), Measured: (814, 708)\n",
      "Tracking max_val: 0.961\n",
      "Predicted: (946, 838), Measured: (799, 727)\n",
      "Tracking max_val: 0.962\n",
      "Predicted: (931, 856), Measured: (783, 747)\n",
      "Tracking max_val: 0.939\n",
      "Predicted: (914, 856), Measured: (766, 767)\n",
      "Tracking max_val: 0.928\n",
      "Predicted: (897, 856), Measured: (749, 787)\n",
      "Tracking max_val: 0.922\n",
      "Predicted: (879, 856), Measured: (731, 808)\n",
      "Tracking max_val: 0.946\n",
      "Predicted: (860, 856), Measured: (713, 829)\n",
      "Tracking max_val: 0.965\n",
      "Predicted: (842, 856), Measured: (695, 850)\n",
      "Tracking max_val: 0.648\n",
      "Predicted: (824, 856), Measured: (680, 856)\n",
      "Tracking max_val: 0.440\n",
      "Predicted: (809, 856), Measured: None\n",
      "Tracking max_val: 0.207\n",
      "Predicted: (793, 856), Measured: None\n",
      "Tracking max_val: 0.186\n",
      "Predicted: (776, 856), Measured: None\n",
      "Tracking max_val: 0.450\n",
      "Predicted: (759, 856), Measured: None\n",
      "Tracking max_val: 0.502\n",
      "Predicted: (742, 856), Measured: None\n",
      "Tracking max_val: 0.526\n",
      "Predicted: (726, 856), Measured: None\n",
      "Tracking max_val: 0.542\n",
      "Predicted: (709, 856), Measured: None\n",
      "Tracking max_val: 0.551\n",
      "Predicted: (692, 856), Measured: None\n",
      "Tracking max_val: 0.560\n",
      "Predicted: (676, 856), Measured: None\n",
      "Tracking max_val: 0.580\n",
      "Predicted: (659, 856), Measured: None\n",
      "Tracking max_val: 0.584\n",
      "Predicted: (642, 856), Measured: None\n",
      "Tracking max_val: 0.380\n",
      "Predicted: (625, 856), Measured: None\n",
      "Tracking max_val: 0.284\n",
      "Predicted: (609, 856), Measured: None\n",
      "Tracking max_val: 0.290\n",
      "Predicted: (592, 856), Measured: None\n",
      "Tracking max_val: 0.291\n",
      "Predicted: (575, 856), Measured: None\n",
      "Tracking max_val: 0.478\n",
      "Predicted: (559, 856), Measured: None\n",
      "Tracking max_val: 0.484\n",
      "Predicted: (542, 856), Measured: None\n",
      "Tracking max_val: 0.484\n",
      "Predicted: (525, 856), Measured: None\n",
      "Tracking max_val: 0.484\n",
      "Predicted: (508, 856), Measured: None\n",
      "Tracking max_val: 0.484\n",
      "Predicted: (492, 856), Measured: None\n"
     ]
    }
   ],
   "source": [
    "video_path = os.path.join(inputs, 'part2.mp4')  # Replace with your input video path\n",
    "template_path = os.path.join(inputs, 'template.png')  # Replace with your template image path\n",
    "output_path = os.path.join(outputs, 'tracked_object.mp4')  # Output video path\n",
    "\n",
    "track(video_path, template_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = display_video(output_path)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compare Bayesian filtering and Kalman filtering (theoretically). **(2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO c):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
